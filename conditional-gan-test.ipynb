{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-29T19:52:32.037875Z","iopub.execute_input":"2022-04-29T19:52:32.038222Z","iopub.status.idle":"2022-04-29T19:52:32.043533Z","shell.execute_reply.started":"2022-04-29T19:52:32.038185Z","shell.execute_reply":"2022-04-29T19:52:32.042565Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom tqdm.auto import tqdm\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\nfrom torchvision.utils import make_grid\nfrom torch.utils.data import DataLoader, Dataset\nimport matplotlib.pyplot as plt\ntorch.manual_seed(0) # Set for our testing purposes, please do not change!\nimport pytorch_lightning as pl\nfrom PIL import Image\n\n\ndef show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28), nrow=5, show=True):\n    '''\n    Function for visualizing images: Given a tensor of images, number of images, and\n    size per image, plots and prints the images in an uniform grid.\n    '''\n    image_tensor = (image_tensor + 1) / 2\n    image_unflat = image_tensor.detach().cpu()\n    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n    if show:\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:55:07.405337Z","iopub.execute_input":"2022-04-29T19:55:07.405626Z","iopub.status.idle":"2022-04-29T19:55:07.416452Z","shell.execute_reply.started":"2022-04-29T19:55:07.405598Z","shell.execute_reply":"2022-04-29T19:55:07.415440Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# set parameters\nDEVICE = 'cuda'\nLEARNING_RATE = 2e-4\nBATCH_SIZE = 16\nNUM_WORKERS = 2\nIMAGE_SIZE = 256\nCHANNELS_IMG = 3\nL1_LAMBDA = 100\nLAMBDA_GP = 10\nNUM_EPOCHS = 3\nSAVE_PER_EPOCHS = 1\nSAVE_MODEL = True\nLOAD_MODEL = False\nCHECKPOINT_DISC = 'disc.pth.tar'\nCHECKPOINT_GEN = 'gen.pth.tar'","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:58:00.631312Z","iopub.execute_input":"2022-04-29T19:58:00.631749Z","iopub.status.idle":"2022-04-29T19:58:00.636688Z","shell.execute_reply.started":"2022-04-29T19:58:00.631718Z","shell.execute_reply":"2022-04-29T19:58:00.636125Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"Generator class","metadata":{}},{"cell_type":"code","source":"class Generator(nn.Module):\n    '''\n    Generator Class\n    Values:\n        input_dim: the dimension of the input vector, a scalar\n        im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n              (MNIST is black-and-white, so 1 channel is your default)\n        hidden_dim: the inner dimension, a scalar\n    '''\n    def __init__(self, input_dim=10, im_chan=1, hidden_dim=64):\n        super(Generator, self).__init__()\n        self.input_dim = input_dim\n        # Build the neural network\n        self.gen = nn.Sequential(\n            self.make_gen_block(input_dim, hidden_dim * 4),\n            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\n            self.make_gen_block(hidden_dim * 2, hidden_dim),\n            self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n        )\n\n    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n        '''\n        Function to return a sequence of operations corresponding to a generator block of DCGAN;\n        a transposed convolution, a batchnorm (except in the final layer), and an activation.\n        Parameters:\n            input_channels: how many channels the input feature representation has\n            output_channels: how many channels the output feature representation should have\n            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n            stride: the stride of the convolution\n            final_layer: a boolean, true if it is the final layer and false otherwise \n                      (affects activation and batchnorm)\n        '''\n        if not final_layer:\n            return nn.Sequential(\n                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n                nn.BatchNorm2d(output_channels),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            return nn.Sequential(\n                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n                nn.Tanh(),\n            )\n\n    def forward(self, noise):\n        '''\n        Function for completing a forward pass of the generator: Given a noise tensor, \n        returns generated images.\n        Parameters:\n            noise: a noise tensor with dimensions (n_samples, input_dim)\n        '''\n        x = noise.view(len(noise), self.input_dim, 1, 1)\n        return self.gen(x)\n\ndef get_noise(n_samples, input_dim, device='cpu'):\n    '''\n    Function for creating noise vectors: Given the dimensions (n_samples, input_dim)\n    creates a tensor of that shape filled with random numbers from the normal distribution.\n    Parameters:\n        n_samples: the number of samples to generate, a scalar\n        input_dim: the dimension of the input vector, a scalar\n        device: the device type\n    '''\n    return torch.randn(n_samples, input_dim, device=device)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:48:15.521882Z","iopub.execute_input":"2022-04-29T19:48:15.522199Z","iopub.status.idle":"2022-04-29T19:48:15.535750Z","shell.execute_reply.started":"2022-04-29T19:48:15.522157Z","shell.execute_reply":"2022-04-29T19:48:15.535011Z"},"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Discriminator","metadata":{}},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    '''\n    Discriminator Class\n    Values:\n      im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n            (MNIST is black-and-white, so 1 channel is your default)\n      hidden_dim: the inner dimension, a scalar\n    '''\n    def __init__(self, im_chan=1, hidden_dim=64):\n        super(Discriminator, self).__init__()\n        self.disc = nn.Sequential(\n            self.make_disc_block(im_chan, hidden_dim),\n            self.make_disc_block(hidden_dim, hidden_dim * 2),\n            self.make_disc_block(hidden_dim * 2, 1, final_layer=True),\n        )\n\n    def make_disc_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n        '''\n        Function to return a sequence of operations corresponding to a discriminator block of the DCGAN; \n        a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer).\n        Parameters:\n            input_channels: how many channels the input feature representation has\n            output_channels: how many channels the output feature representation should have\n            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n            stride: the stride of the convolution\n            final_layer: a boolean, true if it is the final layer and false otherwise \n                      (affects activation and batchnorm)\n        '''\n        if not final_layer:\n            return nn.Sequential(\n                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n                nn.BatchNorm2d(output_channels),\n                nn.LeakyReLU(0.2, inplace=True),\n            )\n        else:\n            return nn.Sequential(\n                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n            )\n\n    def forward(self, image):\n        '''\n        Function for completing a forward pass of the discriminator: Given an image tensor, \n        returns a 1-dimension tensor representing fake/real.\n        Parameters:\n            image: a flattened image tensor with dimension (im_chan)\n        '''\n        disc_pred = self.disc(image)\n        return disc_pred.view(len(disc_pred), -1)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:48:15.537470Z","iopub.execute_input":"2022-04-29T19:48:15.537803Z","iopub.status.idle":"2022-04-29T19:48:15.553602Z","shell.execute_reply.started":"2022-04-29T19:48:15.537766Z","shell.execute_reply":"2022-04-29T19:48:15.552361Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Lightning data module","metadata":{}},{"cell_type":"code","source":"# Sketchs and Colored Image dataset\nclass AnimeDataset(Dataset):\n    def __init__(self,root_dir,transforms):\n        self.root_dir = root_dir\n        self.list_files = os.listdir(self.root_dir)\n        self.transform = transforms\n    def __len__(self):\n        return len(self.list_files)\n    def __getitem__(self, index):\n        # read image file\n        img_file = self.list_files[index]\n        img_path = os.path.join(self.root_dir, img_file)\n        image = np.array(Image.open(img_path))\n        \n        # divide image into sketchs and colored_imgs, right is sketch and left is colored images\n        sketchs = image[:,image.shape[1]//2:,:]\n        colored_imgs = image[:,:image.shape[1]//2,:]\n        \n#         # data augmentation on both sketchs and colored_imgs\n#         augmentations = self.transform.both_transform(image=sketchs, image0=colored_imgs)\n#         sketchs, colored_imgs = augmentations['image'], augmentations['image0']\n        \n#         # conduct data augmentation respectively\n#         sketchs = self.transform.transform_only_input(image=sketchs)['image']\n#         colored_imgs = self.transform.transform_only_mask(image=colored_imgs)['image']\n        return sketchs, colored_imgs","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:55:59.902988Z","iopub.execute_input":"2022-04-29T19:55:59.903687Z","iopub.status.idle":"2022-04-29T19:55:59.911692Z","shell.execute_reply.started":"2022-04-29T19:55:59.903643Z","shell.execute_reply":"2022-04-29T19:55:59.910827Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"train_dataset = AnimeDataset(\n    root_dir='../input/anime-sketch-colorization-pair/data/train/',\n    transforms=[]\n)\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=BATCH_SIZE, \n    shuffle=True, \n    num_workers=NUM_WORKERS\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:58:07.639772Z","iopub.execute_input":"2022-04-29T19:58:07.640070Z","iopub.status.idle":"2022-04-29T19:58:07.651828Z","shell.execute_reply.started":"2022-04-29T19:58:07.640038Z","shell.execute_reply":"2022-04-29T19:58:07.651011Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"next(iter(train_dataset))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:58:08.358801Z","iopub.execute_input":"2022-04-29T19:58:08.359125Z","iopub.status.idle":"2022-04-29T19:58:08.378839Z","shell.execute_reply.started":"2022-04-29T19:58:08.359076Z","shell.execute_reply":"2022-04-29T19:58:08.378180Z"},"trusted":true},"execution_count":27,"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"(array([[[255, 255, 255],\n         [255, 255, 255],\n         [255, 255, 255],\n         ...,\n         [255, 255, 255],\n         [255, 255, 255],\n         [255, 255, 255]],\n \n        [[255, 255, 255],\n         [255, 255, 255],\n         [255, 255, 255],\n         ...,\n         [255, 255, 255],\n         [255, 255, 255],\n         [255, 255, 255]],\n \n        [[255, 255, 255],\n         [255, 255, 255],\n         [255, 255, 255],\n         ...,\n         [255, 255, 255],\n         [255, 255, 255],\n         [255, 255, 255]],\n \n        ...,\n \n        [[255, 255, 255],\n         [255, 255, 255],\n         [255, 255, 255],\n         ...,\n         [255, 255, 255],\n         [255, 255, 255],\n         [255, 255, 255]],\n \n        [[255, 255, 255],\n         [255, 255, 255],\n         [255, 255, 255],\n         ...,\n         [255, 255, 255],\n         [255, 255, 255],\n         [255, 255, 255]],\n \n        [[255, 255, 255],\n         [255, 255, 255],\n         [255, 255, 255],\n         ...,\n         [255, 255, 255],\n         [255, 255, 255],\n         [255, 255, 255]]], dtype=uint8),\n array([[[255, 255, 255],\n         [255, 255, 255],\n         [255, 255, 255],\n         ...,\n         [255, 255, 255],\n         [255, 255, 255],\n         [255, 255, 255]],\n \n        [[255, 255, 255],\n         [255, 255, 255],\n         [255, 255, 255],\n         ...,\n         [255, 255, 255],\n         [255, 255, 255],\n         [255, 255, 255]],\n \n        [[255, 255, 255],\n         [255, 255, 255],\n         [255, 255, 255],\n         ...,\n         [255, 255, 255],\n         [255, 255, 255],\n         [255, 255, 255]],\n \n        ...,\n \n        [[255, 255, 255],\n         [255, 255, 255],\n         [255, 255, 255],\n         ...,\n         [255, 255, 255],\n         [255, 255, 255],\n         [255, 255, 255]],\n \n        [[255, 255, 255],\n         [255, 255, 255],\n         [255, 255, 255],\n         ...,\n         [255, 255, 255],\n         [255, 255, 255],\n         [255, 255, 255]],\n \n        [[255, 255, 255],\n         [255, 255, 255],\n         [255, 255, 255],\n         ...,\n         [255, 255, 255],\n         [255, 255, 255],\n         [255, 255, 255]]], dtype=uint8))"},"metadata":{}}]},{"cell_type":"markdown","source":"Lightning module","metadata":{}},{"cell_type":"code","source":"class GAN(pl.LightningModule):\n    def __init__(\n        self,\n        channels=3,\n        width=IMAGE_SIZE,\n        height=IMAGE_SIZE,\n        latent_dim: int = 100,\n        lr: float = 0.0002,\n        b1: float = 0.5,\n        b2: float = 0.999,\n        batch_size: int = BATCH_SIZE,\n        **kwargs\n    ):\n        super().__init__()\n        self.save_hyperparameters()\n\n        # TODO change\n        data_shape = (channels, width, height)\n        self.generator = Generator(latent_dim=self.hparams.latent_dim, img_shape=data_shape)\n        self.discriminator = Discriminator(img_shape=data_shape)\n\n        self.validation_z = torch.randn(8, self.hparams.latent_dim)\n\n        self.example_input_array = torch.zeros(2, self.hparams.latent_dim)\n\n    def forward(self, z):\n        return self.generator(z)\n\n    def adversarial_loss(self, y_hat, y):\n        return F.binary_cross_entropy(y_hat, y)\n\n    def training_step(self, batch, batch_idx, optimizer_idx):\n        imgs, _ = batch\n\n        # sample noise\n        z = torch.randn(imgs.shape[0], self.hparams.latent_dim)\n        z = z.type_as(imgs)\n\n        # train generator\n        if optimizer_idx == 0:\n\n            # generate images\n            self.generated_imgs = self(z)\n\n            # log sampled images\n            sample_imgs = self.generated_imgs[:6]\n            grid = torchvision.utils.make_grid(sample_imgs)\n            self.logger.experiment.add_image(\"generated_images\", grid, 0)\n\n            # ground truth result (ie: all fake)\n            # put on GPU because we created this tensor inside training_loop\n            valid = torch.ones(imgs.size(0), 1)\n            valid = valid.type_as(imgs)\n\n            # adversarial loss is binary cross-entropy\n            g_loss = self.adversarial_loss(self.discriminator(self(z)), valid)\n            tqdm_dict = {\"g_loss\": g_loss}\n            output = OrderedDict({\"loss\": g_loss, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict})\n            return output\n\n        # train discriminator\n        if optimizer_idx == 1:\n            # Measure discriminator's ability to classify real from generated samples\n\n            # how well can it label as real?\n            valid = torch.ones(imgs.size(0), 1)\n            valid = valid.type_as(imgs)\n\n            real_loss = self.adversarial_loss(self.discriminator(imgs), valid)\n\n            # how well can it label as fake?\n            fake = torch.zeros(imgs.size(0), 1)\n            fake = fake.type_as(imgs)\n\n            fake_loss = self.adversarial_loss(self.discriminator(self(z).detach()), fake)\n\n            # discriminator loss is the average of these\n            d_loss = (real_loss + fake_loss) / 2\n            tqdm_dict = {\"d_loss\": d_loss}\n            output = OrderedDict({\"loss\": d_loss, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict})\n            return output\n\n    def configure_optimizers(self):\n        lr = self.hparams.lr\n        b1 = self.hparams.b1\n        b2 = self.hparams.b2\n\n        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n        return [opt_g, opt_d], []\n\n    def on_epoch_end(self):\n        z = self.validation_z.type_as(self.generator.model[0].weight)\n\n        # log sampled images\n        sample_imgs = self(z)\n        grid = torchvision.utils.make_grid(sample_imgs)\n        if self.current_epoch %2 == 20 : \n            print(\"Images for \", self.current_epoch)\n            show_tensor_images(sample_imgs)\n        self.logger.experiment.add_image(\"generated_images\", grid, self.current_epoch)","metadata":{},"execution_count":null,"outputs":[]}]}